{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import skimage.transform as sk_transform\n",
    "import skimage.filters as sk_filters\n",
    "from skimage.transform import SimilarityTransform\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'SVHN_single_grey1.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-216b67d953f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSVHN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SVHN_single_grey1.h5\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'SVHN_single_grey1.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "SVHN = h5py.File(\"SVHN_single_grey1.h5\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val = SVHN[\"X_train\"][:]\n",
    "y_train_val = SVHN[\"y_train\"][:]\n",
    "X_test = SVHN[\"X_test\"][:]\n",
    "y_test = SVHN[\"y_test\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVHN.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21f5565bc18>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAWw0lEQVR4nO2dX4yc5XXGn+P1euzdtddem7UXG9UEcREUNQatLCSqyE3aiKJIgNREcIG4QHFUBalI6QWiUqFSL0hVQFxUVKZYcSrKnwYQVmW1QVYqlBvCQsGYuG2IMcT/dtder71e/H9PL+azurjfeWb2nZlvDO/zk6yd/c68851553s8s+8z57zm7hBCfPlZ1O0EhBDVILELkQkSuxCZILELkQkSuxCZILELkQmLWxlsZrcDeBpAD4B/dPfH2f1rtZr39fWVxpgFODc3t+AxnbAUzaz0+KJF8f+Z0ZhGsZQ8gPh5R3PYKJY6j1GOixfHl1zKNdAolpI/m9/U17qnp2fBj5mS++zsLM6ePVuaSLLYzawHwN8D+GMABwG8bWY73f3X0Zi+vj5s2bKlNHbhwoXwXGfPni09fu7cuXDMxYsXwxibRHYx9vb2lh5fsmRJOIbFosdrBMsxet7RHALAzMxMGGOvC7u4o+d9zTXXJJ3rzJkzYeyzzz4LY9E1wkTL5nfZsmVhjL3WAwMDC35M9p9YxK5du8JYKx/jNwP4yN33u/t5AC8CuLOFxxNCdJBWxL4ewO/m/X6wOCaEuApp5W/2ss9w/+/zsZltBbAV4B+BhBCdpZV39oMArpv3+wYAh6+8k7tvc/dRdx+t1WotnE4I0QqtiP1tADea2fVmtgTAPQB2tictIUS7Sf4Y7+4XzexBAP+OuvW23d0/ZGPm5ubC1VG2en7p0qXS42xFlVkdbLU1sgYbxVJgf9YsX748jPX394exaNV6amoqHLN06dIwxmCrxdHqP3vN2Pwy54KtdEcr/NPT0+GYY8eOhTE2V6tXrw5j7JqLPvEy1yh6Xswhaclnd/ddAOK1fiHEVYO+QSdEJkjsQmSCxC5EJkjsQmSCxC5EJrS0Gr9Q3D202Jj1FtkJzAZJLUBhtlZkkZw/fz4cw54Xs95WrlwZxgYHB8PY6dOnw1gEs2tSi3Uia4vZdexcrEgmsmaB+LVh88ReM1ZQxMalFLW0uxJU7+xCZILELkQmSOxCZILELkQmSOxCZEKlq/EMVrgSrdKyYhEWYwUXrGAhWgFlbZHYCu2KFSvC2Jo1a5LGRSvrbJWWuRps5Z+tMEeuxsmTJ5Mej10fbDU+et7s8ViM5cjapLEc271SH6F3diEyQWIXIhMkdiEyQWIXIhMkdiEyQWIXIhMqtd4WLVoUFn+wYozIGhoaGgrHsJ1HWM8yZoPMzs6WHmd91RjMXtuwYUMYY9ZhZPWx57Vq1aowxnJkxSlRjFmRzLpKtcMiy4sV3bBrMXUXIma9RddPynZSdOuqMCKE+FIhsQuRCRK7EJkgsQuRCRK7EJkgsQuRCS1Zb2Z2AMAMgEsALrr7KD3Z4sWhXZayJROz10ZGRsIYqxqLtk8CgImJidLjzPphVgizvNhzY7bR5OTkgsewyrbh4eEwxvqxRT3oWNUbm3tGivXJrDwGsxvbDXteKc+5HT77H7p7vDmWEOKqQB/jhciEVsXuAH5uZu+Y2dZ2JCSE6Aytfoy/zd0Pm9kwgDfM7L/c/c35dyj+E9gK8J7sQojO0tI7u7sfLn5OAHgNwOaS+2xz91F3H03dB1wI0TrJYjezfjNbfvk2gG8D2NuuxIQQ7aWVj/FrAbxWWEuLAfyzu/8bG9Db2xtaYinNI1OtN2Y1RZYREFcusWon1hiQPWdmyzHbJbLY2HZY7Fzr168PY8yGmpmZKT0+Pj4ejmFzlWqVRfORamux55y6/VMUY3Z0FGNWb7LY3X0/gK+njhdCVIusNyEyQWIXIhMkdiEyQWIXIhMkdiEyodKGk8x6Yzba6tWrFzxm7dq1YYw1nDx69GgYm5qaKj3OKrkYqXvVMYsqsmSYjZO61xuzmqI5Tm30yGC2XMpjMpuM7evHqgBTbbmIFOtN7+xCZILELkQmSOxCZILELkQmSOxCZEKlq/G1Wg0bN24sjbHClahvHVtVZ7FoCyqA19yzleQItgrOzlWr1cJYSuEN234oZZuhRkTjWJFJao4pMEeDxdj2T6mr8RHsetP2T0KIEIldiEyQ2IXIBIldiEyQ2IXIBIldiEyovBAm6mm2YcOGcFxUFMIsklSLJyXG+pKl9BEDuOXFCifYnESkWm9sjqNxzIJithaD5R+9Zux1ZjmmxmhvuOA6YHMf9RSkvfXCiBDiS4XELkQmSOxCZILELkQmSOxCZILELkQmNLTezGw7gO8AmHD3rxXHhgC8BGAjgAMAvufuJxqebPFiDA8Pl8bYFkRR9Q/rB8ZizJ5i/eSiLY1mZ2fDMWzbJWaTMFvuzJkzYSyyf9hzTrXyUvrasXOlWlcsFuWfWvXGYqmWbpQ/q3qL5rfVqrefALj9imMPA9jt7jcC2F38LoS4imko9mK/9Svbqt4JYEdxeweAu9qclxCizaT+zb7W3Y8AQPGz/LO5EOKqoeMLdGa21czGzGws6rsuhOg8qWIfN7MRACh+TkR3dPdt7j7q7qNReykhROdJFftOAPcXt+8H8Hp70hFCdIpmrLcXAGwBsMbMDgJ4FMDjAF42swcAfArgu82crKenJ6xgY5ZMVFUWWWEAMDk5GcbOnz8fxk6ciB3EY8eOlR4/depUOIZt48RsOdZwMsoDiG05VpnHGiWyWF9fXxiL8k+tAmTWFXs9U86VUs0HtN/eZOeKmqZSOzeM/F8i9wahbzUaK4S4etA36ITIBIldiEyQ2IXIBIldiEyQ2IXIhEobTl66dCm0qVjlWGT/HDx4MBxz+PDhMMZsKFYtNzFR/t0hZk+lVlClVqJFFhWrKGPzwWytqPKKPWZq1Rt7zinNOVOaOQLp1iGzxFKIzqW93oQQErsQuSCxC5EJErsQmSCxC5EJErsQmVC59TY9PR3GIo4fP156fP/+/eEYZsulNP8DEObOLKNOVFAxovOx58ysN2aVpcZSxjALkL1mEayZI7PemN3IrLeUhpns+ojyl/UmhJDYhcgFiV2ITJDYhcgEiV2ITKh0Nf7ChQthgQorJon6yX3yySfhmKhoBeCrpmwlluWYQsoqMpCef0TKyjnAV4tTCj86sfIfzVXqHLLnnOquRDE2h1qNF0KESOxCZILELkQmSOxCZILELkQmSOxCZEIz2z9tB/AdABPu/rXi2GMAvg/gsif2iLvvavRY586dw8cff1waY73fou2Ojh49Go5hNtng4GAYS7XDIlJ7rrHClZQCCQY7V2ovvJR5TOmtB6QVIrH8mC3H5j712kmx3jrVg+4nAG4vOf6Uu28q/jUUuhCiuzQUu7u/CUAbqwvxBaeVv9kfNLM9ZrbdzFa1LSMhREdIFfszAG4AsAnAEQBPRHc0s61mNmZmY6dPn048nRCiVZLE7u7j7n7J3ecAPAtgM7nvNncfdffRgYGB1DyFEC2SJHYzG5n3690A9rYnHSFEp2jGensBwBYAa8zsIIBHAWwxs00AHMABAD9o5mQXLlzAoUOHSmPsI/6JEydKj588eTIcw+wTZk+xWPSYzJ5ilhGzB1N7rkUxlse5c+eS8kixDlOtSNYnj9lh0WvGbK3UvoGMlGsktRoxoqHY3f3eksPPtTULIUTH0TfohMgEiV2ITJDYhcgEiV2ITJDYhciEShtOXrx4MdxCaXZ2NhwX2XLMmmAWWq1WC2PMsossGWYZMXttZmYmjDE7jJGSI6s4ZK8L+5JUlD+z8pi9xvJnjxldB6m2FrM9mS13NVhvemcXIhMkdiEyQWIXIhMkdiEyQWIXIhMkdiEyoVLrbW5uLrRyUqq8mIW2YsWKMNbf3x/GGJFFwiwjZmudOXMmjKVWeUUxVvXG8mDWG7MHI8sx1UJj52LzEZ2v3bZWozxYM83otU7ZH46N0Tu7EJkgsQuRCRK7EJkgsQuRCRK7EJlQ6Wq8u4crrmyVNipOWbp0aThm2bJlYWzJkiVhjBWuRCvTnVhxZ/PBinyiQhi2Gs9WwVNXyKNx7e5pB/B5jFbdU1a6Ab7iznrXMaLnxuaD5Rihd3YhMkFiFyITJHYhMkFiFyITJHYhMkFiFyITmtn+6ToAPwWwDsAcgG3u/rSZDQF4CcBG1LeA+p67l+/TNI/ICmG93/r6+kqPDw4OhmOWL18exphVE201BcTWCsud2ScsxmwcZl9FtiKz61gvvKmpqTC2bt26MBbZoszaTCVlrtjrPD4+HsaiaxHg1yOb/6jQK2UMo5l39osAfuTuXwVwK4AfmtlNAB4GsNvdbwSwu/hdCHGV0lDs7n7E3d8tbs8A2AdgPYA7Aewo7rYDwF2dSlII0ToL+pvdzDYCuBnAWwDWuvsRoP4fAoDhdicnhGgfTYvdzAYAvALgIXc/tYBxW81szMzGOvH3mhCiOZoSu5n1oi7059391eLwuJmNFPERABNlY919m7uPuvso+y67EKKzNBS71Zf9ngOwz92fnBfaCeD+4vb9AF5vf3pCiHbRTNXbbQDuA/CBmb1XHHsEwOMAXjazBwB8CuC7jR7I3ZP6bUUwG4TZMcwqS7E7WD8zZq+xijhWSceq9qIcU6veWI6s6i0ax/Kg/dMSK8qi6409Lwabe3bttLtaLnqdmSXXUOzu/ksA0SN8q5nEhBDdR9+gEyITJHYhMkFiFyITJHYhMkFiFyITKt/+KbJCmEUVWTzMBhkaGgpjbGso9i2/ycnJ0uPMOmEVdtPT02GMVWWtXr06jEX2D7N+UiqogLTtn5j1xmLM3mSxaP5Z7uz1ZF8MY/PIYtFrw6zIlEaaemcXIhMkdiEyQWIXIhMkdiEyQWIXIhMkdiEyoVLrzcxCW4PZHZGNc/r06XAMs3FYdRKriItsDfZ4zBY6efJkGDt27FgYY800a7Va6XGWY+r+ayn74rExqQ042bjoGmFNNlOtNzbHKfsSMruOXd8RemcXIhMkdiEyQWIXIhMkdiEyQWIXIhMqXY1ftGgRBgYGSmOsMCFaLWarsKwAha1ksjyix2SrsIxTp+KO3MePHw9j1157bRiLiipSHYNoVR0AJiZKGwoDAA4dOlR6nPXWSy3WafdqPHM7OrEaHzkojOgaViGMEEJiFyIXJHYhMkFiFyITJHYhMkFiFyITGlpvZnYdgJ8CWAdgDsA2d3/azB4D8H0AlxuzPeLuuxo8VlhkwGyLaJsnZmekFk6kbHfErCv2vJgNNTU1FcZYAU2KjcNsSlaQwwqRPv3009LjzPZM3eKJFfKk9MJjeaRcp0Ba7zqWY/S82LXYjM9+EcCP3P1dM1sO4B0ze6OIPeXuf9fEYwghukwze70dAXCkuD1jZvsArO90YkKI9rKgz01mthHAzQDeKg49aGZ7zGy7ma1qc25CiDbStNjNbADAKwAecvdTAJ4BcAOATai/8z8RjNtqZmNmNsYaFwghOktTYjezXtSF/ry7vwoA7j7u7pfcfQ7AswA2l411923uPuruo2yRQgjRWRqK3epLhc8B2OfuT847PjLvbncD2Nv+9IQQ7aKZ1fjbANwH4AMze6849giAe81sEwAHcADADxo9kLuHNgm1DIK+cGwbp6i6DkivrooqilhlWGpFHLPeWEVcNCfMTmLbaDGbklmAka3IbL7Unmvsz8PofOxTJrt2mLXZCSs4IrqG2Rw2sxr/SwBlj0A9dSHE1YW+QSdEJkjsQmSCxC5EJkjsQmSCxC5EJlTacHJubi7JeousIWZ1sAokFhscHAxj/f39pceZFcaeF2NycjKMHT58OIxFNhRrpMlyZJV5zPKKxrGqwtQKwZRtwFhTSWbpMmsr1TqMYinVdywHvbMLkQkSuxCZILELkQkSuxCZILELkQkSuxCZUKn1BsQ2D6uGiiwIZjMwG2fZsmVhbOXKlQuOsT3bUpohAtwOGx8fD2ORxcZyZHYYs7zYY0aNKlmFF9unjFUWsn3bosdklW2RxdoIZq+lWMspe8cxu07v7EJkgsQuRCZI7EJkgsQuRCZI7EJkgsQuRCZUar25e2hBMBsnssqYVcOsFVbZxqyyaBxr2MgsI2ZDMRsn2kcNiCvw2FyxirhUyyuaR2YNpZ6LzVV0HbDrg72eDHbtMJs4stFS7GNZb0IIiV2IXJDYhcgEiV2ITJDYhciEhqvxZrYUwJsAasX9f+buj5rZ9QBeBDAE4F0A97l7vBxZEK1As4KLaIWRFbSsWbMmKcZWpqPzsV54rMCHnSva8grghTCR28H6qrEVXFYkw9yEaE5YQcj09HQYY0VDKdcB60HHVsHZqjp7bswxiJySlEKYVnvQnQPwTXf/OurbM99uZrcC+DGAp9z9RgAnADzQxGMJIbpEQ7F7ncvtO3uLfw7gmwB+VhzfAeCujmQohGgLze7P3lPs4DoB4A0AvwUw7e6XP8cdBLC+MykKIdpBU2J390vuvgnABgCbAXy17G5lY81sq5mNmdkY+4aREKKzLGg13t2nAfwHgFsBrDSzy6tIGwCU7lzg7tvcfdTdR1O/hiiEaJ2GYjeza8xsZXF7GYA/ArAPwC8A/Glxt/sBvN6pJIUQrdNMIcwIgB1m1oP6fw4vu/u/mtmvAbxoZn8D4D8BPNfMCSNrYGhoKBwzPDxcenzdunXhGGatsO2farVaGIssHtazjJ2LFaf09PSEMUZk9TFrk+XPLEBmvUV/srHnzGCfCtkcp9il7Dmn9pljllj0WjPrLWX7p4Zid/c9AG4uOb4f9b/fhRBfAPQNOiEyQWIXIhMkdiEyQWIXIhMkdiEywVKtkKSTmU0C+KT4dQ2A8j2CqkV5fB7l8Xm+aHn8nrtfUxaoVOyfO7HZmLuPduXkykN5ZJiHPsYLkQkSuxCZ0E2xb+viueejPD6P8vg8X5o8uvY3uxCiWvQxXohM6IrYzex2M/tvM/vIzB7uRg5FHgfM7AMze8/Mxio873YzmzCzvfOODZnZG2b2m+Lnqi7l8ZiZHSrm5D0zu6OCPK4zs1+Y2T4z+9DM/rw4XumckDwqnRMzW2pmvzKz94s8/ro4fr2ZvVXMx0tmtrAGEe5e6T8APai3tfoKgCUA3gdwU9V5FLkcALCmC+f9BoBbAOydd+xvATxc3H4YwI+7lMdjAP6i4vkYAXBLcXs5gP8BcFPVc0LyqHROABiAgeJ2L4C3UG8Y8zKAe4rj/wDgzxbyuN14Z98M4CN33+/11tMvArizC3l0DXd/E8DUFYfvRL1xJ1BRA88gj8px9yPu/m5xewb15ijrUfGckDwqxeu0vclrN8S+HsDv5v3ezWaVDuDnZvaOmW3tUg6XWevuR4D6RQegvGNHNTxoZnuKj/kd/3NiPma2EfX+CW+hi3NyRR5AxXPSiSav3RB7WSuNblkCt7n7LQD+BMAPzewbXcrjauIZADegvkfAEQBPVHViMxsA8AqAh9w93sO7+jwqnxNvoclrRDfEfhDAdfN+D5tVdhp3P1z8nADwGrrbeWfczEYAoPg50Y0k3H28uNDmADyLiubEzHpRF9jz7v5qcbjyOSnLo1tzUpx7wU1eI7oh9rcB3FisLC4BcA+AnVUnYWb9Zrb88m0A3wawl4/qKDtRb9wJdLGB52VxFdyNCubE6o3TngOwz92fnBeqdE6iPKqek441ea1qhfGK1cY7UF/p/C2Av+xSDl9B3Ql4H8CHVeYB4AXUPw5eQP2TzgMAVgPYDeA3xc+hLuXxTwA+ALAHdbGNVJDHH6D+kXQPgPeKf3dUPSckj0rnBMDvo97EdQ/q/7H81bxr9lcAPgLwLwBqC3lcfYNOiEzQN+iEyASJXYhMkNiFyASJXYhMkNiFyASJXYhMkNiFyASJXYhM+F9ttURXtM3UyAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train_val[25], cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_val[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33600, 32, 32)\n",
      "(33600,)\n",
      "(8400, 32, 32)\n",
      "(8400,)\n",
      "(18000, 32, 32)\n",
      "(18000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.2)\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print (X_val.shape)\n",
    "print (y_val.shape)\n",
    "print (X_test.shape)\n",
    "print (y_test.shape)\n",
    "train_dim = X_train.shape[0]\n",
    "test_dim = X_test.shape[0]\n",
    "val_dim = X_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, in_, out_):\n",
    "        self.W = np.random.randn(in_,out_) * 0.01\n",
    "        self.b = np.zeros((1,out_))\n",
    "        self.params = [self.W, self.b]\n",
    "        self.gradW = None\n",
    "        self.gradb = None\n",
    "        self.gradInput = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.output = np.dot(X, self.W) + self.b\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, nextgrad):\n",
    "        self.gradW = np.dot(self.X.T, nextgrad)\n",
    "        self.gradb = np.sum(nextgrad, axis = 0)\n",
    "        self.gradInput = np.dot(nextgrad, self.W.T)\n",
    "        return self.gradInput, [self.gradW, self.gradb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.gradInput = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.output = (X - np.mean(X))/(np.std(X)+0.0000001)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, nextgrad):\n",
    "        return nextgrad, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.gradInput = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.output = np.maximum(X,0)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, nextgrad):\n",
    "        self.gradInput = nextgrad.copy()\n",
    "        self.gradInput [self.output<=0] = 0 \n",
    "        return self.gradInput, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exp_x = np.exp(X - np.max(X, axis = 1, keepdims = True))\n",
    "    return exp_x/np.sum(exp_x, axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy():\n",
    "    def forward(self, X, y):\n",
    "        self.m = y.shape[0]\n",
    "        self.p = softmax(X)\n",
    "        cross_entropy = -np.log(self.p[range(self.m),y])\n",
    "        loss = np.sum(cross_entropy) /self.m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        y_idx = y.argmax()\n",
    "        grad = softmax(X)\n",
    "        grad[range(self.m),y] -= 1\n",
    "        grad /=self.m\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, lossfunc = CrossEntropy()):\n",
    "        self.params = []\n",
    "        self.layers = []\n",
    "        self.loss_func = lossfunc\n",
    "        self.grads = []\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        self.params.append(layer.params)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, nextgrad):\n",
    "        self.clear_grad_param()\n",
    "        for layer in reversed(self.layers):\n",
    "            nextgrad, grad = layer.backward(nextgrad)\n",
    "            self.grads.append(grad)\n",
    "        return self.grads\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        out = self.forward(X)\n",
    "        loss = self.loss_func.forward(out, y)\n",
    "        nextgrad = self.loss_func.backward(out, y)\n",
    "        grads = self.backward(nextgrad)\n",
    "        return loss, grads\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.forward(X)\n",
    "        return np.argmax(X, axis = 1)\n",
    "\n",
    "    def predict_scores(self, X):\n",
    "        X = self.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def clear_grad_param(self):\n",
    "        self.grads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(velocity, params, grads, learning_rate = 0.01, mu = 0.9, epoch=1):\n",
    "    for v, p, g, in zip(velocity, params, reversed(grads)):\n",
    "        for i in range(len(g)):\n",
    "            v[i] = mu * v[i] + learning_rate * g[i]\n",
    "            p[i] -= v[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch (X, y, minibatch_size):\n",
    "    n = X.shape[0]\n",
    "    minibatches = []\n",
    "    permutation = np.random.permutation(X.shape[0])\n",
    "    X = X[permutation]\n",
    "    y = y[permutation]\n",
    "    \n",
    "    for i in range(0, n, minibatch_size):\n",
    "        X_batch = X[i:i + minibatch_size, :]\n",
    "        X_batch = (X_batch - np.mean(X_batch))/(np.std(X_batch)+0.0000001)\n",
    "        y_batch = y[i:i + minibatch_size, ]\n",
    "        minibatches.append((X_batch, y_batch))\n",
    "    \n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X_train, y_train, minibatch_size, epoch, learning_rate, mu = 0.9, X_val=None, y_val = None):\n",
    "    val_loss_epoch = []\n",
    "    minibatches = minibatch(X_train, y_train, minibatch_size)\n",
    "    minibatches_val = minibatch(X_val, y_val, minibatch_size)    \n",
    "    for i in range (epoch):\n",
    "        loss_batch = []\n",
    "        val_loss_batch = []\n",
    "        velocity = []\n",
    "        for param_layer in net.params:\n",
    "            p = [np.zeros_like(param) for param in list(param_layer)]\n",
    "            velocity.append(p)\n",
    "            \n",
    "        for X_mini, y_mini in minibatches:\n",
    "            loss, grads = net.train_step(X_mini, y_mini)\n",
    "            loss_batch.append(loss)\n",
    "            update_params(velocity, net.params, grads, learning_rate = learning_rate, mu = mu, epoch = epoch)\n",
    "        \n",
    "        for X_mini_val, y_mini_val in minibatches_val:\n",
    "            val_loss, _ = net.train_step(X_mini, y_mini)\n",
    "            val_loss_batch.append(val_loss)\n",
    "        \n",
    "        m_train = X_train.shape[0]\n",
    "        m_val = X_val.shape[0]\n",
    "        \n",
    "        y_train_pred = np.array([], dtype = \"int64\")\n",
    "        y_val_pred = np.array([], dtype = \"int64\")\n",
    "        y_train1 = []\n",
    "        y_val1 = []        \n",
    "        for i in range(0, m_train, minibatch_size):\n",
    "            X_tr = X_train[i:i + minibatch_size, :]\n",
    "            y_tr = y_train[i:i + minibatch_size, ]\n",
    "            y_train1 = np.append(y_train1, y_tr)\n",
    "            y_train_pred = np.append(y_train_pred, net.predict(X_tr) )\n",
    "        \n",
    "        for i in range(0, m_val, minibatch_size):\n",
    "            X_va = X_val[i:i + minibatch_size, :]\n",
    "            y_va = y_val[i:i + minibatch_size, ]\n",
    "            y_val1 = np.append(y_val1, y_va)\n",
    "            y_val_pred = np.append(y_val_pred, net.predict(X_va) )\n",
    "            \n",
    "        train_acc = check_accuracy(y_train1, y_train_pred)\n",
    "        val_acc = check_accuracy(y_train1, y_train_pred)\n",
    "        \n",
    "        mean_train_loss = sum(loss_batch)/float(len(loss_batch))\n",
    "        mean_val_loss = sum(val_loss_batch)/float(len(val_loss_batch))\n",
    "        \n",
    "        val_loss_epoch.append(mean_val_loss)\n",
    "        print (\"validation loss\", mean_val_loss, \" Validation Accuracy \", val_acc)\n",
    "        #if mean_val_loss < np.min(val_loss_epoch):\n",
    "         #   best_net = net.deepcopy()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(y_true, y_pred):\n",
    "    return (np.mean(y_true==y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flip = np.fliplr(X_train)\n",
    "tform = SimilarityTransform(translation=(0, 2))\n",
    "X_train_warp_up = []\n",
    "j=0\n",
    "for i in range(X_train.shape[0]):    \n",
    "    X_train_warp_up.append(sk_transform.warp(X_train[i], tform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tform = SimilarityTransform(translation=(0, -2))\n",
    "X_train_warp_down = []\n",
    "j=0\n",
    "for i in range(X_train.shape[0]):    \n",
    "    X_train_warp_down.append(sk_transform.warp(X_train[i], tform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tform = SimilarityTransform(translation=(2, 0))\n",
    "X_train_warp_left = []\n",
    "j=0\n",
    "for i in range(X_train.shape[0]):    \n",
    "    X_train_warp_left.append(sk_transform.warp(X_train[i], tform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tform = SimilarityTransform(translation=(-2, 0))\n",
    "X_train_warp_right = []\n",
    "j=0\n",
    "for i in range(X_train.shape[0]):    \n",
    "    X_train_warp_right.append(sk_transform.warp(X_train[i], tform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_warp_up = np.array(X_train_warp_up)\n",
    "X_train_warp_right = np.array(X_train_warp_right)\n",
    "X_train_warp_left = np.array(X_train_warp_left)\n",
    "X_train_warp_down = np.array(X_train_warp_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21f5fb46320>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train_warp_right[24].reshape(32,32), cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(train_dim,1024)\n",
    "X_train_warp_right = X_train_warp_right.reshape(train_dim,1024)\n",
    "X_train_warp_left = X_train_warp_left.reshape(train_dim,1024)\n",
    "X_train_warp_up = X_train_warp_up.reshape(train_dim,1024)\n",
    "X_train_warp_down = X_train_warp_down.reshape(train_dim,1024)\n",
    "X_test = X_test.reshape(test_dim,1024)\n",
    "X_val = X_val.reshape(val_dim,1024)\n",
    "X_train = (X_train - np.mean(X_train))/(np.std(X_train)+0.0000001)\n",
    "X_test = (X_test - np.mean(X_test))/(np.std(X_test)+0.0000001)\n",
    "X_val = (X_val - np.mean(X_val))/(np.std(X_val)+0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 1.1696257330151756  Validation Accuracy  0.61625\n",
      "validation loss 1.0349163189923192  Validation Accuracy  0.6788690476190476\n",
      "validation loss 0.962369355681732  Validation Accuracy  0.7152678571428571\n",
      "validation loss 0.9012315607991398  Validation Accuracy  0.7295833333333334\n",
      "validation loss 0.8762709298747081  Validation Accuracy  0.7429464285714286\n",
      "validation loss 0.8263990063302143  Validation Accuracy  0.7582440476190476\n",
      "validation loss 0.8220505364896661  Validation Accuracy  0.7626785714285714\n",
      "validation loss 0.7982625578230851  Validation Accuracy  0.773125\n",
      "validation loss 0.7945331781076649  Validation Accuracy  0.7767857142857143\n",
      "validation loss 0.7668182086118847  Validation Accuracy  0.7861309523809524\n",
      "validation loss 0.7510635876933484  Validation Accuracy  0.794047619047619\n",
      "validation loss 0.7382926888104973  Validation Accuracy  0.7974702380952381\n",
      "validation loss 0.7332663834852882  Validation Accuracy  0.8033928571428571\n",
      "validation loss 0.7358392177126014  Validation Accuracy  0.8041964285714286\n",
      "validation loss 0.729419613873689  Validation Accuracy  0.8066666666666666\n",
      "validation loss 0.7123898580954153  Validation Accuracy  0.8127380952380953\n",
      "validation loss 0.7121150418997917  Validation Accuracy  0.813422619047619\n",
      "validation loss 0.7147825161880668  Validation Accuracy  0.8132440476190477\n",
      "validation loss 0.7218346925496952  Validation Accuracy  0.8082738095238096\n",
      "validation loss 0.7310813940905565  Validation Accuracy  0.8059226190476191\n",
      "validation loss 0.620254683721287  Validation Accuracy  0.8272321428571429\n",
      "validation loss 0.6397434832490348  Validation Accuracy  0.8276488095238095\n",
      "validation loss 0.638562898281747  Validation Accuracy  0.8290773809523809\n",
      "validation loss 0.644607111894917  Validation Accuracy  0.8302083333333333\n",
      "validation loss 0.6371086623520191  Validation Accuracy  0.8308035714285714\n",
      "validation loss 0.639257822307214  Validation Accuracy  0.8321726190476191\n",
      "validation loss 0.6353142871153852  Validation Accuracy  0.8354166666666667\n",
      "validation loss 0.6320249442884851  Validation Accuracy  0.8373214285714285\n",
      "validation loss 0.6396130589428292  Validation Accuracy  0.8361309523809524\n",
      "validation loss 0.6268904870012812  Validation Accuracy  0.8397023809523809\n",
      "validation loss 0.637909387919719  Validation Accuracy  0.8399107142857143\n",
      "validation loss 0.6476782920140084  Validation Accuracy  0.8390178571428571\n",
      "validation loss 0.6560457243760477  Validation Accuracy  0.8394345238095238\n",
      "validation loss 0.6590343748833546  Validation Accuracy  0.839047619047619\n",
      "validation loss 0.6637983992346186  Validation Accuracy  0.8400595238095238\n",
      "validation loss 0.6592441533072319  Validation Accuracy  0.84375\n",
      "validation loss 0.6747817659154542  Validation Accuracy  0.8369047619047619\n",
      "validation loss 0.6676504477025789  Validation Accuracy  0.8394047619047619\n",
      "validation loss 0.6623992173174396  Validation Accuracy  0.8425297619047619\n",
      "validation loss 0.6722384028685366  Validation Accuracy  0.8434226190476191\n",
      "validation loss 0.7837923243582676  Validation Accuracy  0.5791071428571428\n",
      "validation loss 0.7266925746432252  Validation Accuracy  0.6316964285714286\n",
      "validation loss 0.6896431992159854  Validation Accuracy  0.6598214285714286\n",
      "validation loss 0.662283220852468  Validation Accuracy  0.7266369047619048\n",
      "validation loss 0.6526228197824417  Validation Accuracy  0.7041369047619047\n",
      "validation loss 0.6422385778708881  Validation Accuracy  0.748125\n",
      "validation loss 0.6348553521202353  Validation Accuracy  0.7314880952380952\n",
      "validation loss 0.6289430533437105  Validation Accuracy  0.7535416666666667\n",
      "validation loss 0.6266681898612411  Validation Accuracy  0.7576488095238095\n",
      "validation loss 0.6265960868305236  Validation Accuracy  0.7605059523809524\n",
      "validation loss 0.6202255708045196  Validation Accuracy  0.7869047619047619\n",
      "validation loss 0.6201537589770929  Validation Accuracy  0.7849404761904762\n",
      "validation loss 0.6192871286132869  Validation Accuracy  0.7974107142857143\n",
      "validation loss 0.6211107759171297  Validation Accuracy  0.8000297619047619\n",
      "validation loss 0.6210721796666371  Validation Accuracy  0.803422619047619\n",
      "validation loss 0.6166868863604547  Validation Accuracy  0.7945535714285714\n",
      "validation loss 0.6160789814861809  Validation Accuracy  0.7938392857142857\n",
      "validation loss 0.6139931048602583  Validation Accuracy  0.7894642857142857\n",
      "validation loss 0.611386713153112  Validation Accuracy  0.787797619047619\n",
      "validation loss 0.612459201344169  Validation Accuracy  0.7944642857142857\n",
      "validation loss 1.319935142657346  Validation Accuracy  0.2225892857142857\n",
      "validation loss 0.9916504540946979  Validation Accuracy  0.47345238095238096\n",
      "validation loss 0.8708845701152458  Validation Accuracy  0.6607440476190476\n",
      "validation loss 0.8148494379332335  Validation Accuracy  0.702827380952381\n",
      "validation loss 0.7755831891179759  Validation Accuracy  0.7336607142857143\n",
      "validation loss 0.7508435464914205  Validation Accuracy  0.7448511904761905\n",
      "validation loss 0.7322671851009096  Validation Accuracy  0.7460119047619047\n",
      "validation loss 0.7226329063471566  Validation Accuracy  0.7429761904761905\n",
      "validation loss 0.7097963191569481  Validation Accuracy  0.7421130952380952\n",
      "validation loss 0.6987448424954816  Validation Accuracy  0.7524107142857143\n",
      "validation loss 0.6850524383098147  Validation Accuracy  0.7633928571428571\n",
      "validation loss 0.6731389798129123  Validation Accuracy  0.7768154761904762\n",
      "validation loss 0.6680152496646048  Validation Accuracy  0.777797619047619\n",
      "validation loss 0.6664794387243144  Validation Accuracy  0.7786011904761905\n",
      "validation loss 0.6631116845692712  Validation Accuracy  0.7794345238095238\n",
      "validation loss 0.6509386602657975  Validation Accuracy  0.7875892857142858\n",
      "validation loss 0.6463677626640509  Validation Accuracy  0.7896130952380952\n",
      "validation loss 0.640598147731417  Validation Accuracy  0.7933928571428571\n",
      "validation loss 0.6371353112044698  Validation Accuracy  0.8014583333333334\n",
      "validation loss 0.6375297888555813  Validation Accuracy  0.8000297619047619\n",
      "validation loss 0.866754351260015  Validation Accuracy  0.6301488095238095\n",
      "validation loss 0.7925380195913255  Validation Accuracy  0.6743154761904762\n",
      "validation loss 0.7614523067133765  Validation Accuracy  0.6763988095238095\n",
      "validation loss 0.7355448434927271  Validation Accuracy  0.7004166666666667\n",
      "validation loss 0.7135757034486366  Validation Accuracy  0.7175595238095238\n",
      "validation loss 0.7018966702081677  Validation Accuracy  0.7329166666666667\n",
      "validation loss 0.6884965892101976  Validation Accuracy  0.7357142857142858\n",
      "validation loss 0.6758898684826632  Validation Accuracy  0.7447321428571428\n",
      "validation loss 0.6675946449792793  Validation Accuracy  0.7521130952380952\n",
      "validation loss 0.6602648769245613  Validation Accuracy  0.7544642857142857\n",
      "validation loss 0.6552432380077795  Validation Accuracy  0.7624702380952381\n",
      "validation loss 0.6506363046396136  Validation Accuracy  0.7569642857142858\n",
      "validation loss 0.6426479483996567  Validation Accuracy  0.7594940476190476\n",
      "validation loss 0.6386993295312485  Validation Accuracy  0.7597619047619047\n",
      "validation loss 0.6345436724049145  Validation Accuracy  0.760297619047619\n",
      "validation loss 0.6306717007553325  Validation Accuracy  0.7565773809523809\n",
      "validation loss 0.6290734099469246  Validation Accuracy  0.7633630952380952\n",
      "validation loss 0.6273556059621814  Validation Accuracy  0.7670833333333333\n",
      "validation loss 0.6249209857036444  Validation Accuracy  0.7716071428571428\n",
      "validation loss 0.6223631058225638  Validation Accuracy  0.7702083333333334\n",
      "validation loss 0.8668209437536109  Validation Accuracy  0.6400595238095238\n",
      "validation loss 0.7786123205733161  Validation Accuracy  0.7126190476190476\n",
      "validation loss 0.7358995577155484  Validation Accuracy  0.7309821428571428\n",
      "validation loss 0.70933180750574  Validation Accuracy  0.7316071428571429\n",
      "validation loss 0.6843865874828198  Validation Accuracy  0.7448214285714285\n",
      "validation loss 0.6724874126510977  Validation Accuracy  0.7485714285714286\n",
      "validation loss 0.66271751483168  Validation Accuracy  0.7613690476190477\n",
      "validation loss 0.6517110835729659  Validation Accuracy  0.7680357142857143\n",
      "validation loss 0.6460839007619715  Validation Accuracy  0.7795238095238095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.6419801581132011  Validation Accuracy  0.7858333333333334\n",
      "validation loss 0.6383850849264218  Validation Accuracy  0.7840178571428571\n",
      "validation loss 0.6358832894270058  Validation Accuracy  0.7882440476190476\n",
      "validation loss 0.633541597169427  Validation Accuracy  0.7878869047619048\n",
      "validation loss 0.6304171603475456  Validation Accuracy  0.7885416666666667\n",
      "validation loss 0.6274566120890992  Validation Accuracy  0.7914583333333334\n",
      "validation loss 0.6277502852978881  Validation Accuracy  0.7912797619047619\n",
      "validation loss 0.6268288073442206  Validation Accuracy  0.793422619047619\n",
      "validation loss 0.6264907299690774  Validation Accuracy  0.7918452380952381\n",
      "validation loss 0.6254873452079065  Validation Accuracy  0.7916369047619047\n",
      "validation loss 0.6254471175646756  Validation Accuracy  0.7885119047619048\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "\n",
    "iterations = 10\n",
    "learning_rate = 0.1\n",
    "hidden_nodes = 48\n",
    "output_nodes = 10\n",
    "\n",
    "nn = NN()\n",
    "nn.add_layer(Linear(input_dim, hidden_nodes))\n",
    "nn.add_layer(Normalize())\n",
    "nn.add_layer(ReLU())\n",
    "nn.add_layer(Normalize())\n",
    "nn.add_layer(Linear(hidden_nodes, output_nodes))\n",
    "\n",
    "random_value = np.random.randint(100)*0.01\n",
    "nn = train(nn, X_train, y_train, minibatch_size=600, epoch = 20, learning_rate= learning_rate, X_val = X_val, y_val = y_val)\n",
    "nn = train(nn, X_train+random_value, y_train, minibatch_size=600, epoch = 20, learning_rate= learning_rate, X_val = X_val, y_val = y_val)\n",
    "nn = train(nn, X_train_warp_right, y_train, minibatch_size=600, epoch = 20, learning_rate= learning_rate, X_val = X_val, y_val = y_val)\n",
    "nn = train(nn, X_train_warp_up, y_train, minibatch_size=600, epoch = 20, learning_rate= learning_rate, X_val = X_val, y_val = y_val)\n",
    "nn = train(nn, X_train_warp_left, y_train, minibatch_size=600, epoch = 20, learning_rate= learning_rate, X_val = X_val, y_val = y_val)\n",
    "nn = train(nn, X_train_warp_down, y_train, minibatch_size=600, epoch = 20, learning_rate= learning_rate, X_val = X_val, y_val = y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.72      0.77      1814\n",
      "           1       0.75      0.76      0.76      1828\n",
      "           2       0.72      0.81      0.76      1803\n",
      "           3       0.57      0.73      0.64      1719\n",
      "           4       0.81      0.80      0.80      1812\n",
      "           5       0.78      0.75      0.76      1768\n",
      "           6       0.79      0.67      0.73      1832\n",
      "           7       0.89      0.77      0.83      1808\n",
      "           8       0.72      0.72      0.72      1812\n",
      "           9       0.71      0.75      0.73      1804\n",
      "\n",
      "    accuracy                           0.75     18000\n",
      "   macro avg       0.76      0.75      0.75     18000\n",
      "weighted avg       0.76      0.75      0.75     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensorflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3ae37383d41e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorically\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_test1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorically\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_val1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorically\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tensorflow' is not defined"
     ]
    }
   ],
   "source": [
    "y_train1 = tensorflow.keras.utils.to_categorically(y_train, num_classes = 10)\n",
    "y_test1 = tensorflow.keras.utils.to_categorically(y_test, num_classes = 10)\n",
    "y_val1 = tensorflow.keras.utils.to_categorically(y_val, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
